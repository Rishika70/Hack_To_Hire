{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1cCGI5AqxtqUxhvkdHgAA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishika70/Hack_To_Hire/blob/main/Hack_To_Hire.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**"
      ],
      "metadata": {
        "id": "HdRUYqYvpbvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface"
      ],
      "metadata": {
        "id": "S3mWHne4JEW4",
        "outputId": "1496f328-6e76-4645-8af9-f7aa584a0892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: huggingface\n",
            "Successfully installed huggingface-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjyVTvSpGy4y",
        "outputId": "867c42df-79b4-46f1-c2f7-cc61508d3548"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSylZ2PIq_kD",
        "outputId": "3196eb66-ad7c-4b09-a152-66eebccc936a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Z-rrKZ4i2bMF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import T5Tokenizer, T5Model\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import BertTokenizer, DataCollatorWithPadding\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objs as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import html\n",
        "from collections import Counter\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer, create_optimizer\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rMsT19Yj6J2",
        "outputId": "0e584acd-cc8a-47b3-cb24-e5725ddcc8ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "bfuStHmKfmBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"toughdata/quora-question-answer-dataset\")\n",
        "df = pd.DataFrame(dataset['train'])  # Convert the dataset to a DataFrame\n"
      ],
      "metadata": {
        "id": "CqhLSuyDkwJX"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split dataset"
      ],
      "metadata": {
        "id": "vR0m1jJbf1iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_split = dataset['train'].train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split['train']\n",
        "validation_dataset = train_test_split['test']"
      ],
      "metadata": {
        "id": "-hUj00uofxzb"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analyze the Data**"
      ],
      "metadata": {
        "id": "wyMLY0X2lKNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ1QfVShlGWs",
        "outputId": "d8603a02-6f6a-49a6-b294-078411ad398a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            question  \\\n",
            "0  Why whenever I get in the shower my girlfriend...   \n",
            "1            What is a proxy, and how can I use one?   \n",
            "2  What song has the lyrics \"someone left the cak...   \n",
            "3  I am the owner of an adult website called http...   \n",
            "4  Does the Bible mention anything about a place ...   \n",
            "\n",
            "                                              answer  \n",
            "0  Isnâ€™t it awful? You would swear that there was...  \n",
            "1  A proxy server is a system or router that prov...  \n",
            "2                                 MacArthur's Park\\n  \n",
            "3  Don't let apps that are liers put adds on your...  \n",
            "4  St. John in the book of Revelation mentions an...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocqc-8Y2ljuQ",
        "outputId": "422c470b-d5f7-4a5f-e041-06d9e33f19ff"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 56402 entries, 0 to 56401\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   question  56402 non-null  object\n",
            " 1   answer    56402 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 881.4+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ8Ph8RJlmCb",
        "outputId": "2709e610-2ff8-49a6-9f1f-f87ac875c398"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 question answer\n",
            "count                                               56402  56402\n",
            "unique                                               3234  54726\n",
            "top     Would Hillary Clinton have made a better Presi...   No\\n\n",
            "freq                                                  106     89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QRa0ldJmREt",
        "outputId": "6336c746-d693-4227-c122-47fd91129019"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['question', 'answer'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove irrelevant information\n"
      ],
      "metadata": {
        "id": "6IltDyC0lrmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Pre-Processing**"
      ],
      "metadata": {
        "id": "8XfQ1ivxpBG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean the text , remove urls, Special Characters , stopwords and Lemmatize\n",
        "\n"
      ],
      "metadata": {
        "id": "djQgGIADoR8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  # Remove URLs\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "  # Remove special characters and convert to lowercase\n",
        "  text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stop words and lemmatize\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "  # Join the tokens back into a string\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "# Apply the preprocessing function to the 'question' and 'answer' columns\n",
        "df['question'] = df['question'].apply(preprocess_text)\n",
        "df['answer'] = df['answer'].apply(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtxwIeVrnnLA",
        "outputId": "fb08333b-5ca6-416f-f204-1c5842789c47"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['question', 'answer'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "PGyiZ6fwoA-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT and Metrice Evaluation"
      ],
      "metadata": {
        "id": "RDQbId-0pnR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "KuPcvXdqoYcf"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "UkN9yxC1tvmw"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune the model on the Quora dataset"
      ],
      "metadata": {
        "id": "_pipIuygqaSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# DataFrame with questions and answers\n",
        "data = {\n",
        "    \"question\": [\"What is AI?\", \"How does a car work?\"],\n",
        "    \"answer\": [\"Artificial Intelligence\", \"Through an internal combustion engine\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "def preprocess_function(df):\n",
        "    questions = df[\"question\"].str.lower().tolist()\n",
        "    answers = df[\"answer\"].str.lower().tolist()\n",
        "\n",
        "    # Tokenize the questions and answers\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    tokenized_examples = tokenizer(questions, answers, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "\n",
        "    # Convert tensors to lists and add to DataFrame\n",
        "    df['input_ids'] = tokenized_examples['input_ids'].tolist()\n",
        "    df['attention_mask'] = tokenized_examples['attention_mask'].tolist()\n",
        "    df['token_type_ids'] = tokenized_examples['token_type_ids'].tolist()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the preprocessing function to the DataFrame\n",
        "tokenized_df = preprocess_function(df)\n",
        "\n",
        "print(tokenized_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svBfo7avq_ML",
        "outputId": "2b741885-d4f4-41bd-829a-673ea9e97ea8"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               question                                 answer  \\\n",
            "0           What is AI?                Artificial Intelligence   \n",
            "1  How does a car work?  Through an internal combustion engine   \n",
            "\n",
            "                                           input_ids  \\\n",
            "0  [101, 2054, 2003, 9932, 1029, 102, 7976, 4454,...   \n",
            "1  [101, 2129, 2515, 1037, 2482, 2147, 1029, 102,...   \n",
            "\n",
            "                                      attention_mask  \\\n",
            "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
            "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
            "\n",
            "                                      token_type_ids  \n",
            "0  [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
            "1  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For question-answer pairs:\n",
        "train_encodings = tokenizer(train_df['question'].tolist(), train_df['answer'].tolist(), return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "test_encodings = tokenizer(test_df['question'].tolist(), test_df['answer'].tolist(), return_tensors='pt', max_length=512, padding='max_length', truncation=True)"
      ],
      "metadata": {
        "id": "Wwq52EAV_SJ3"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "tokenized_df = preprocess_function(df)"
      ],
      "metadata": {
        "id": "Sq922UCmxwrr"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class for our data\n",
        "class QuoraDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Convert labels to numerical representation (assuming labels are strings)\n",
        "        item['labels'] = torch.tensor(0 if self.labels[idx].lower() == 'no' else 1)  # Example: 'no' -> 0, other -> 1\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "v4c1PXCwsVA-"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders for training and testing\n",
        "train_dataset = QuoraDataset(train_encodings, train_df['answer'].tolist())\n",
        "test_dataset = QuoraDataset(test_encodings, test_df['answer'].tolist())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "WDx0_pjzsqG7"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train The Model"
      ],
      "metadata": {
        "id": "vKunlVzCyIdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Assuming binary classification\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3  # Adjust as needed\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits  # Access the logits from the model's output\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: Training Loss = {avg_train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZAvfWx7-jZ5",
        "outputId": "2927006a-7b53-4a9f-fa06-b6eda64ce88b"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-160-b8ee25297b90>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3: Training Loss = 0.4018\n",
            "Epoch 2/3: Training Loss = 0.4979\n",
            "Epoch 3/3: Training Loss = 0.2931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "5S8CrzoKuU7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OSZ3vNP0t5a",
        "outputId": "d379ef8c-c120-4716-a1c9-57b45af1f1ca"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-160-b8ee25297b90>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STATE-OF-THE-ART-LLM**"
      ],
      "metadata": {
        "id": "3JDxuOR5drDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT Model"
      ],
      "metadata": {
        "id": "Ijg0LwDgximL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "class GPT2ForSequenceClassification(GPT2PreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifier(hidden_states[:, -1, :])  # Use the last hidden state for classification\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, logits) if loss is not None else (logits,)\n",
        "\n",
        "# Assuming you have a binary classification task (e.g., Question/Answers)\n",
        "num_labels = 2\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=num_labels)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=2e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh67Dlgk8w0L",
        "outputId": "0c02fb0a-4ccf-490c-93d9-783ceab9ab8f"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Pre-processing"
      ],
      "metadata": {
        "id": "Ol99XrU0HEkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_function(df):\n",
        "    questions = df[\"question\"].str.lower().tolist()\n",
        "    answers = df[\"answer\"].str.lower().tolist()\n",
        "\n",
        "    # Tokenize the questions and answers\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    tokenized_examples = tokenizer(questions, answers, truncation=True, padding='max_length', max_length=128, return_tensors='pt')  # Adjust max_length as needed\n",
        "\n",
        "    # Convert tensors to lists and add to DataFrame\n",
        "    df['input_ids'] = tokenized_examples['input_ids'].tolist()\n",
        "    df['attention_mask'] = tokenized_examples['attention_mask'].tolist()\n",
        "    df['token_type_ids'] = tokenized_examples['token_type_ids'].tolist()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the preprocessing function to the DataFrame\n",
        "tokenized_df = preprocess_function(df)\n",
        "\n",
        "print(tokenized_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYEZDa6EF8TT",
        "outputId": "ffd087d2-dad1-40ef-a3d5-6791fbd00707"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               question                                 answer  \\\n",
            "0           What is AI?                Artificial Intelligence   \n",
            "1  How does a car work?  Through an internal combustion engine   \n",
            "\n",
            "                                           input_ids  \\\n",
            "0  [101, 2054, 2003, 9932, 1029, 102, 7976, 4454,...   \n",
            "1  [101, 2129, 2515, 1037, 2482, 2147, 1029, 102,...   \n",
            "\n",
            "                                      attention_mask  \\\n",
            "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
            "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
            "\n",
            "                                      token_type_ids  \n",
            "0  [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
            "1  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:  # Use test_loader for evaluation\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[0]  # Access logits from the tuple returned by GPT2ForSequenceClassification\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCd06JCuHfA6",
        "outputId": "e081a5a6-3762-423c-fe7d-2b23c979af75"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-160-b8ee25297b90>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_answers = [\"This is a predicted answer.\", \"Another predicted answer.\"]\n",
        "reference_answers = [[\"This is the reference answer.\"], [\"The other reference answer.\"]]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_scores = []\n",
        "for pred, ref in zip(predicted_answers, reference_answers):\n",
        "  bleu_score = sentence_bleu(ref, pred)\n",
        "  bleu_scores.append(bleu_score)\n",
        "\n",
        "# Print or use the BLEU scores as needed\n",
        "print(bleu_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UhhQ0pWJm3j",
        "outputId": "99ad1682-479a-4185-f439-707e120d2919"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5141996115613456, 0.46625954410634507]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **T5 Model**"
      ],
      "metadata": {
        "id": "UzsgPYXu1Muo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a T5 model and tokenizer\n",
        "model = T5Model.from_pretrained('t5-small')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "# Add special tokens to the tokenizer\n",
        "special_tokens = ['<extra_id_0>', '<extra_id_1>']  # Example special tokens\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "\n",
        "# Resize the model's embedding layer to accommodate the new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Now, when you fine-tune or train the model, the embeddings for the special tokens will be updated along with the rest of the model parameters.\n"
      ],
      "metadata": {
        "id": "UbCKsR721qRE",
        "outputId": "96a9fafa-b523-4529-bdba-d2323f951125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(32100, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_function(df):\n",
        "    questions = df[\"question\"].str.lower().tolist()\n",
        "    answers = df[\"answer\"].str.lower().tolist()\n",
        "\n",
        "    # Tokenize the questions and answers\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    tokenized_examples = tokenizer(questions, answers, truncation=True, padding='max_length', max_length=128, return_tensors='pt')  # Adjust max_length as needed\n",
        "\n",
        "    # Convert tensors to lists and add to DataFrame\n",
        "    df['input_ids'] = tokenized_examples['input_ids'].tolist()\n",
        "    df['attention_mask'] = tokenized_examples['attention_mask'].tolist()\n",
        "    df['token_type_ids'] = tokenized_examples['token_type_ids'].tolist()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the preprocessing function to the DataFrame\n",
        "tokenized_df = preprocess_function(df)\n",
        "\n",
        "print(tokenized_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9QvEpgRNjZp",
        "outputId": "045861ae-cc85-4140-d1f2-c4a117ad9065"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               question                                 answer  \\\n",
            "0           What is AI?                Artificial Intelligence   \n",
            "1  How does a car work?  Through an internal combustion engine   \n",
            "\n",
            "                                           input_ids  \\\n",
            "0  [101, 2054, 2003, 9932, 1029, 102, 7976, 4454,...   \n",
            "1  [101, 2129, 2515, 1037, 2482, 2147, 1029, 102,...   \n",
            "\n",
            "                                      attention_mask  \\\n",
            "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
            "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
            "\n",
            "                                      token_type_ids  \n",
            "0  [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
            "1  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders for training and testing\n",
        "train_dataset = QuoraDataset(train_encodings, train_df['answer'].tolist())\n",
        "test_dataset = QuoraDataset(test_encodings, test_df['answer'].tolist())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "CHatnTBkOXNG"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class for our data\n",
        "class QuoraDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Convert labels to numerical representation (assuming labels are strings)\n",
        "        item['labels'] = torch.tensor(0 if self.labels[idx].lower() == 'no' else 1)  # Example: 'no' -> 0, other -> 1\n",
        "        # Add EOS token if it's not present\n",
        "        if self.encodings['input_ids'][idx][-1] != tokenizer.eos_token_id:\n",
        "            item['input_ids'] = torch.cat((item['input_ids'], torch.tensor([tokenizer.eos_token_id])))\n",
        "            item['attention_mask'] = torch.cat((item['attention_mask'], torch.tensor([1])))\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "7HsHhFXzNq8P"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add special tokens to the tokenizer\n",
        "special_tokens = ['<extra_id_0>', '<extra_id_1>']  # Example special tokens\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "\n",
        "# Resize the model's embedding layer to accommodate the new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "def generate_answer(question):\n",
        "  # Tokenize the input question\n",
        "  inputs = tokenizer.encode(\"question: \" + question + \" </s>\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "  # Generate the answer\n",
        "  outputs = model.generate(inputs, max_length=100, num_beams=4, early_stopping=True)\n",
        "\n",
        "  # Decode the generated answer\n",
        "  answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "2fpC2sihQUBW"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "question = \"What is the capital of France?\"\n",
        "generated_answer = generate_answer(question)\n",
        "print(generated_answer)  # Output: Paris"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyLCpzo6QXw_",
        "outputId": "cf7ec850-a669-4714-9cf1-f6343ecdbdf4"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "France\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization"
      ],
      "metadata": {
        "id": "Ve-7B5aB5XN-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZ1OaBxWSKuT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}