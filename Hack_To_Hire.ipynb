{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMiRIx0hNviU9jOaIhQIv6n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishika70/Hack_To_Hire/blob/main/Hack_To_Hire.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**"
      ],
      "metadata": {
        "id": "HdRUYqYvpbvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface"
      ],
      "metadata": {
        "id": "S3mWHne4JEW4",
        "outputId": "1496f328-6e76-4645-8af9-f7aa584a0892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: huggingface\n",
            "Successfully installed huggingface-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjyVTvSpGy4y",
        "outputId": "867c42df-79b4-46f1-c2f7-cc61508d3548"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSylZ2PIq_kD",
        "outputId": "3196eb66-ad7c-4b09-a152-66eebccc936a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Z-rrKZ4i2bMF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import T5Tokenizer, T5Model\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import BertTokenizer, DataCollatorWithPadding\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objs as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import html\n",
        "from collections import Counter\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer, create_optimizer\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rMsT19Yj6J2",
        "outputId": "0e584acd-cc8a-47b3-cb24-e5725ddcc8ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "bfuStHmKfmBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"toughdata/quora-question-answer-dataset\")\n",
        "df = pd.DataFrame(dataset['train'])  # Convert the dataset to a DataFrame\n"
      ],
      "metadata": {
        "id": "CqhLSuyDkwJX"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split dataset"
      ],
      "metadata": {
        "id": "vR0m1jJbf1iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_split = dataset['train'].train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split['train']\n",
        "validation_dataset = train_test_split['test']"
      ],
      "metadata": {
        "id": "-hUj00uofxzb"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analyze the Data**"
      ],
      "metadata": {
        "id": "wyMLY0X2lKNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ1QfVShlGWs",
        "outputId": "d8603a02-6f6a-49a6-b294-078411ad398a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            question  \\\n",
            "0  Why whenever I get in the shower my girlfriend...   \n",
            "1            What is a proxy, and how can I use one?   \n",
            "2  What song has the lyrics \"someone left the cak...   \n",
            "3  I am the owner of an adult website called http...   \n",
            "4  Does the Bible mention anything about a place ...   \n",
            "\n",
            "                                              answer  \n",
            "0  Isnâ€™t it awful? You would swear that there was...  \n",
            "1  A proxy server is a system or router that prov...  \n",
            "2                                 MacArthur's Park\\n  \n",
            "3  Don't let apps that are liers put adds on your...  \n",
            "4  St. John in the book of Revelation mentions an...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocqc-8Y2ljuQ",
        "outputId": "422c470b-d5f7-4a5f-e041-06d9e33f19ff"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 56402 entries, 0 to 56401\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   question  56402 non-null  object\n",
            " 1   answer    56402 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 881.4+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ8Ph8RJlmCb",
        "outputId": "2709e610-2ff8-49a6-9f1f-f87ac875c398"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 question answer\n",
            "count                                               56402  56402\n",
            "unique                                               3234  54726\n",
            "top     Would Hillary Clinton have made a better Presi...   No\\n\n",
            "freq                                                  106     89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QRa0ldJmREt",
        "outputId": "6336c746-d693-4227-c122-47fd91129019"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['question', 'answer'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove irrelevant information\n"
      ],
      "metadata": {
        "id": "6IltDyC0lrmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Pre-Processing**"
      ],
      "metadata": {
        "id": "8XfQ1ivxpBG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean the text , remove urls, Special Characters , stopwords and Lemmatize\n",
        "\n"
      ],
      "metadata": {
        "id": "djQgGIADoR8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  # Remove URLs\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "  # Remove special characters and convert to lowercase\n",
        "  text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stop words and lemmatize\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "  # Join the tokens back into a string\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "# Apply the preprocessing function to the 'question' and 'answer' columns\n",
        "df['question'] = df['question'].apply(preprocess_text)\n",
        "df['answer'] = df['answer'].apply(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtxwIeVrnnLA",
        "outputId": "fb08333b-5ca6-416f-f204-1c5842789c47"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['question', 'answer'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "PGyiZ6fwoA-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT and Metrice Evaluation"
      ],
      "metadata": {
        "id": "RDQbId-0pnR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "KuPcvXdqoYcf"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "UkN9yxC1tvmw"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune the model on the Quora dataset"
      ],
      "metadata": {
        "id": "_pipIuygqaSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# DataFrame with questions and answers\n",
        "data = {\n",
        "    \"question\": [\"What is AI?\", \"How does a car work?\"],\n",
        "    \"answer\": [\"Artificial Intelligence\", \"Through an internal combustion engine\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "def preprocess_function(df):\n",
        "    questions = df[\"question\"].str.lower().tolist()\n",
        "    answers = df[\"answer\"].str.lower().tolist()\n",
        "\n",
        "    # Tokenize the questions and answers\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    tokenized_examples = tokenizer(questions, answers, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "\n",
        "    # Convert tensors to lists and add to DataFrame\n",
        "    df['input_ids'] = tokenized_examples['input_ids'].tolist()\n",
        "    df['attention_mask'] = tokenized_examples['attention_mask'].tolist()\n",
        "    df['token_type_ids'] = tokenized_examples['token_type_ids'].tolist()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the preprocessing function to the DataFrame\n",
        "tokenized_df = preprocess_function(df)\n",
        "\n",
        "print(tokenized_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svBfo7avq_ML",
        "outputId": "2b741885-d4f4-41bd-829a-673ea9e97ea8"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               question                                 answer  \\\n",
            "0           What is AI?                Artificial Intelligence   \n",
            "1  How does a car work?  Through an internal combustion engine   \n",
            "\n",
            "                                           input_ids  \\\n",
            "0  [101, 2054, 2003, 9932, 1029, 102, 7976, 4454,...   \n",
            "1  [101, 2129, 2515, 1037, 2482, 2147, 1029, 102,...   \n",
            "\n",
            "                                      attention_mask  \\\n",
            "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
            "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
            "\n",
            "                                      token_type_ids  \n",
            "0  [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
            "1  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For question-answer pairs:\n",
        "train_encodings = tokenizer(train_df['question'].tolist(), train_df['answer'].tolist(), return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "test_encodings = tokenizer(test_df['question'].tolist(), test_df['answer'].tolist(), return_tensors='pt', max_length=512, padding='max_length', truncation=True)"
      ],
      "metadata": {
        "id": "Wwq52EAV_SJ3"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "tokenized_df = preprocess_function(df)"
      ],
      "metadata": {
        "id": "Sq922UCmxwrr"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class for our data\n",
        "class QuoraDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Convert labels to numerical representation (assuming labels are strings)\n",
        "        item['labels'] = torch.tensor(0 if self.labels[idx].lower() == 'no' else 1)  # Example: 'no' -> 0, other -> 1\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "v4c1PXCwsVA-"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders for training and testing\n",
        "train_dataset = QuoraDataset(train_encodings, train_df['answer'].tolist())\n",
        "test_dataset = QuoraDataset(test_encodings, test_df['answer'].tolist())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "WDx0_pjzsqG7"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train The Model"
      ],
      "metadata": {
        "id": "vKunlVzCyIdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Assuming binary classification\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3  # Adjust as needed\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits  # Access the logits from the model's output\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: Training Loss = {avg_train_loss:.4f}\")"
      ],
      "metadata": {
        "id": "pZAvfWx7-jZ5",
        "outputId": "2927006a-7b53-4a9f-fa06-b6eda64ce88b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-160-b8ee25297b90>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3: Training Loss = 0.4018\n",
            "Epoch 2/3: Training Loss = 0.4979\n",
            "Epoch 3/3: Training Loss = 0.2931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "5S8CrzoKuU7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "id": "9OSZ3vNP0t5a",
        "outputId": "d379ef8c-c120-4716-a1c9-57b45af1f1ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-160-b8ee25297b90>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: From above process implement GPt 2 and T5\n",
        "\n",
        "# **GPT-2 Implementation**\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings_gpt2 = gpt2_tokenizer(train_df['question'].tolist(), return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "test_encodings_gpt2 = gpt2_tokenizer(test_df['question'].tolist(), return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = QuoraDataset(train_encodings, train_df['answer'].tolist())\n",
        "test_dataset = QuoraDataset(test_encodings, test_df['answer'].tolist())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False\n",
        "\n",
        "# Fine-tune GPT-2 (similar structure to BERT fine-tuning)\n",
        "# ... (Implementation for fine-tuning GPT-2)\n",
        "\n",
        "# Evaluate GPT-2\n",
        "# ... (Implementation for evaluating GPT-2)\n",
        "\n",
        "# **T5 Implementation**\n",
        "\n",
        "# Load pre-trained T5 model and tokenizer\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "t5_model = T5Model.from_pretrained('t5-small')\n",
        "\n",
        "# Tokenize the data (T5 expects input and target text)\n",
        "train_encodings_t5 = t5_tokenizer(train_df['question'].tolist(), text_target=train_df['answer'].tolist(), return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "test_encodings_t5 = t5_tokenizer(test_df['question'].tolist(), text_target=test_df['answer'].tolist(), return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset_t5 = QuoraDataset(train_encodings_t5, train_df['answer'].tolist())\n",
        "test_dataset_t5 = QuoraDataset(test_encodings_t5, test_df['answer'].tolist())\n",
        "\n",
        "train_loader_t5 = torch.utils.data.DataLoader(train_dataset_t5, batch_size=16, shuffle=True)\n",
        "test_loader_t5 = torch.utils.data.DataLoader(test_dataset_t5, batch_size=16, shuffle=False)\n",
        "\n",
        "# Fine-tune T5 (similar structure to BERT fine-tuning)\n",
        "# ... (Implementation for fine-tuning T5)\n",
        "\n",
        "# Evaluate T5\n",
        "# ... (Implementation for evaluating T5)\n"
      ],
      "metadata": {
        "id": "LQWYN4T06u8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STATE-OF-THE-ART-LLM**"
      ],
      "metadata": {
        "id": "3JDxuOR5drDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT Model"
      ],
      "metadata": {
        "id": "Ijg0LwDgximL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "class GPT2ForSequenceClassification(GPT2PreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifier(hidden_states[:, -1, :])  # Use the last hidden state for classification\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, logits) if loss is not None else (logits,)\n",
        "\n",
        "# Assuming you have a binary classification task (e.g., Question/Answers)\n",
        "num_labels = 2\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=num_labels)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=2e-5)\n"
      ],
      "metadata": {
        "id": "oh67Dlgk8w0L",
        "outputId": "0c02fb0a-4ccf-490c-93d9-783ceab9ab8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Pre-processing"
      ],
      "metadata": {
        "id": "Ol99XrU0HEkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_function(df):\n",
        "    questions = df[\"question\"].str.lower().tolist()\n",
        "    answers = df[\"answer\"].str.lower().tolist()\n",
        "\n",
        "    # Tokenize the questions and answers\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    tokenized_examples = tokenizer(questions, answers, truncation=True, padding='max_length', max_length=128, return_tensors='pt')  # Adjust max_length as needed\n",
        "\n",
        "    # Convert tensors to lists and add to DataFrame\n",
        "    df['input_ids'] = tokenized_examples['input_ids'].tolist()\n",
        "    df['attention_mask'] = tokenized_examples['attention_mask'].tolist()\n",
        "    df['token_type_ids'] = tokenized_examples['token_type_ids'].tolist()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the preprocessing function to the DataFrame\n",
        "tokenized_df = preprocess_function(df)\n",
        "\n",
        "print(tokenized_df)\n"
      ],
      "metadata": {
        "id": "QYEZDa6EF8TT",
        "outputId": "ffd087d2-dad1-40ef-a3d5-6791fbd00707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               question                                 answer  \\\n",
            "0           What is AI?                Artificial Intelligence   \n",
            "1  How does a car work?  Through an internal combustion engine   \n",
            "\n",
            "                                           input_ids  \\\n",
            "0  [101, 2054, 2003, 9932, 1029, 102, 7976, 4454,...   \n",
            "1  [101, 2129, 2515, 1037, 2482, 2147, 1029, 102,...   \n",
            "\n",
            "                                      attention_mask  \\\n",
            "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
            "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
            "\n",
            "                                      token_type_ids  \n",
            "0  [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
            "1  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:  # Use test_loader for evaluation\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[0]  # Access logits from the tuple returned by GPT2ForSequenceClassification\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "id": "tCd06JCuHfA6",
        "outputId": "e081a5a6-3762-423c-fe7d-2b23c979af75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-160-b8ee25297b90>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Calculate Bleu from above code\n",
        "\n",
        "# Assuming we have generated predictions and have reference answers\n",
        "# For example:\n",
        "predicted_answers = [\"This is a predicted answer.\", \"Another predicted answer.\"]\n",
        "reference_answers = [[\"This is the reference answer.\"], [\"The other reference answer.\"]]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_scores = []\n",
        "for pred, ref in zip(predicted_answers, reference_answers):\n",
        "  bleu_score = sentence_bleu(ref, pred)\n",
        "  bleu_scores.append(bleu_score)\n",
        "\n",
        "# Print or use the BLEU scores as needed\n",
        "print(bleu_scores)\n"
      ],
      "metadata": {
        "id": "6UhhQ0pWJm3j",
        "outputId": "99ad1682-479a-4185-f439-707e120d2919",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5141996115613456, 0.46625954410634507]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **T5 Model**"
      ],
      "metadata": {
        "id": "UzsgPYXu1Muo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a T5 model and tokenizer\n",
        "model = T5Model.from_pretrained('t5-small')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "# Add special tokens to the tokenizer\n",
        "special_tokens = ['<extra_id_0>', '<extra_id_1>']  # Example special tokens\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "\n",
        "# Set the model to training mode and define loss and optimizer\n",
        "model.train()\n",
        "criterion = nn.CrossEntropyLoss()  # Not strictly necessary as T5ForSequenceClassification has built-in loss\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Use the model directly for both forward pass and loss calculation\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "\n",
        "# Resize the model's embedding layer to accommodate the new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Now, when you fine-tune or train the model, the embeddings for the special tokens will be updated along with the rest of the model parameters.\n"
      ],
      "metadata": {
        "id": "UbCKsR721qRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForSequenceClassification\n",
        "# Use T5ForSequenceClassification for sequence classification tasks\n",
        "model = T5ForSequenceClassification.from_pretrained('t5-small')\n",
        "\n",
        "# Add special tokens to the tokenizer (if needed)\n",
        "special_tokens = ['<extra_id_0>', '<extra_id_1>']\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set the model to training mode and define loss and optimizer\n",
        "model.train()\n",
        "criterion = nn.CrossEntropyLoss()  # Not strictly necessary as T5ForSequenceClassification has built-in loss\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Use the model directly for both forward pass and loss calculation\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n"
      ],
      "metadata": {
        "id": "Srg9JnuvKGxG",
        "outputId": "089b11b7-802f-4357-ae58-3ab9e5432311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-160-b8ee25297b90>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index -1 is out of bounds for dimension 1 with size 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-219-32255f7eb621>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Use the model directly for both forward pass and loss calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2072\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All examples must have the same number of <eos> tokens.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2074\u001b[0;31m         \u001b[0msentence_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meos_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_representation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization"
      ],
      "metadata": {
        "id": "Ve-7B5aB5XN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter Plot"
      ],
      "metadata": {
        "id": "AWePmFh05VGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a scatter plot\n",
        "fig = px.scatter(x=range(len(token_embeddings)), y=token_embeddings,\n",
        "                 labels={'x': 'Embedding Dimension', 'y': 'Value'},\n",
        "                 title='T5 Embeddings for the First Token')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "zS-BBPyXSLcX",
        "outputId": "3475a9cf-0807-40f8-b349-bb6e90974a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"84ac2090-c087-4183-bd6e-0a47728739db\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"84ac2090-c087-4183-bd6e-0a47728739db\")) {                    Plotly.newPlot(                        \"84ac2090-c087-4183-bd6e-0a47728739db\",                        [{\"hovertemplate\":\"Embedding Dimension=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767],\"xaxis\":\"x\",\"y\":[-0.07810764759778976,-0.18066732585430145,-0.1542903035879135,-0.10693222284317017,-0.30641722679138184,-0.6268917918205261,-0.003950638230890036,0.15170960128307343,0.21237975358963013,-0.1762062907218933,-0.10841536521911621,0.04211407154798508,-0.17102739214897156,-0.05836804211139679,0.22909162938594818,-0.035116326063871384,-0.02383670024573803,0.2858792543411255,0.30493324995040894,-0.2790749669075012,0.030496858060359955,-0.33315759897232056,-0.1427937000989914,-0.2328324317932129,-0.17145968973636627,-0.0916113629937172,0.034851446747779846,-0.28772690892219543,-0.09413183480501175,0.07531637698411942,-0.10538157820701599,0.15156190097332,-0.19783417880535126,-0.2945212125778198,0.3519677519798279,0.059413231909275055,0.28413352370262146,0.15515336394309998,0.21807651221752167,-0.0016147263813763857,-0.23334667086601257,0.10819905251264572,0.00013353295798879117,-0.033792391419410706,-0.1116265207529068,-0.4875395596027374,-2.7257235050201416,-0.21803826093673706,-0.16720649600028992,-0.4851084351539612,0.044613730162382126,-0.25049397349357605,0.12112441658973694,0.5385537147521973,-0.08410670608282089,0.3322349488735199,-0.2996579110622406,0.4228537678718567,0.18317575752735138,-0.17707869410514832,0.10296838730573654,-0.07946250587701797,0.03147530555725098,-0.22785966098308563,-0.19130007922649384,0.38363638520240784,-0.038057997822761536,0.20198705792427063,-0.11381158232688904,0.435788631439209,-0.7137483358383179,-0.15952777862548828,0.5025801658630371,0.040716324001550674,0.1010061651468277,-0.14264875650405884,-0.05398736149072647,0.14464004337787628,-0.4310542047023773,-0.08443687111139297,-0.17107577621936798,0.5388285517692566,-0.029077840968966484,-0.12274277955293655,0.20896220207214355,0.10360396653413773,-0.4427913427352905,-0.19450867176055908,0.29603707790374756,0.34909239411354065,-0.18255046010017395,0.27603405714035034,-0.02771063521504402,0.24484248459339142,0.21420614421367645,0.08048580586910248,0.19908945262432098,-0.019951967522501945,0.23695777356624603,0.3250499665737152,0.23162241280078888,-0.05187899246811867,0.22563378512859344,-0.5709481239318848,0.1991405487060547,-0.2860080301761627,-0.1056298017501831,-0.14043833315372467,0.05133761465549469,-2.8443875312805176,0.21886934340000153,0.21407552063465118,-0.37220659852027893,-0.03442449867725372,-0.436689555644989,0.52834153175354,0.29532375931739807,0.01558278314769268,0.048717278987169266,0.30862700939178467,-0.2537613809108734,0.45253604650497437,-0.19485262036323547,-0.2985808253288269,-0.14542865753173828,0.23327286541461945,0.6095420122146606,0.17541640996932983,0.3106883466243744,0.24924173951148987,0.179908886551857,0.6582603454589844,0.25619712471961975,-0.07284616678953171,-0.09927818179130554,-0.11429858952760696,0.36968740820884705,0.009334573522210121,-0.2615433633327484,-0.2595529854297638,-0.09756439924240112,-0.379523903131485,-2.8821287155151367,0.03677469864487648,0.19872687757015228,0.026435334235429764,-0.353898286819458,0.0059613739140331745,0.24910295009613037,0.24468378722667694,0.009767191484570503,0.3128918409347534,-0.11924299597740173,-0.04123244062066078,-0.16451287269592285,-0.15384328365325928,-0.11189741641283035,-0.2506983280181885,0.34629327058792114,0.3918270468711853,0.21217529475688934,-0.08820471912622452,0.264487087726593,-0.17084021866321564,-0.11632820218801498,0.2234700322151184,0.7180708646774292,0.024317672476172447,0.0564485527575016,-0.0437767468392849,-0.3542148470878601,0.23087970912456512,0.23831123113632202,0.0748981460928917,0.1804170459508896,0.11456387490034103,-0.03414037823677063,0.3440778851509094,0.49509456753730774,-0.15485422313213348,-0.3220635950565338,0.39274224638938904,0.04794793203473091,-0.04770008102059364,0.4740985631942749,0.09009508043527603,0.5964609980583191,0.01434869971126318,-0.4787602126598358,0.3840891718864441,-0.04535939171910286,0.005495590623468161,0.03653792664408684,-0.09046118706464767,0.5297853946685791,0.2387574315071106,0.09186607599258423,-0.4885062873363495,-0.13768717646598816,0.49074605107307434,-0.06991242617368698,0.018030356615781784,-0.22296449542045593,0.03200311213731766,-0.009276507422327995,3.9570393562316895,0.18743276596069336,-0.19585788249969482,0.4090646505355835,0.18586114048957825,-0.4117731750011444,0.34482449293136597,-0.35837605595588684,-0.32052868604660034,0.05161486193537712,0.21590186655521393,0.1358465850353241,0.13260778784751892,0.17112453281879425,-0.20001031458377838,0.33521947264671326,0.13966386020183563,-0.21315065026283264,0.38510990142822266,-0.23787574470043182,0.10199487209320068,0.1902265101671219,0.9965056777000427,0.18355630338191986,-1.4682884216308594,0.05795514956116676,0.2846468687057495,-0.30144375562667847,0.2783818542957306,-0.050745610147714615,-0.19805432856082916,-0.09189357608556747,-0.29133716225624084,0.038184475153684616,0.04929659143090248,0.03534536808729172,0.4681300222873688,-0.11651857942342758,0.2084801346063614,-0.1819981336593628,0.21472102403640747,0.24618281424045563,-0.05690554156899452,0.34966418147087097,-0.214243546128273,0.4634842276573181,0.1232636347413063,-0.09460419416427612,-0.3414964973926544,0.19839918613433838,0.2387361228466034,0.11849747598171234,0.3509752154350281,-0.4350479245185852,0.15199515223503113,-0.28028547763824463,-0.15921524167060852,-0.034767188131809235,0.2652636170387268,-0.44205692410469055,-0.2885097563266754,-0.08907463401556015,-0.022639792412519455,-0.03176923468708992,-0.3198654353618622,0.06924979388713837,-0.4918838143348694,-0.28334617614746094,-4.017383575439453,0.014217720367014408,0.026960929855704308,0.12970764935016632,0.29100513458251953,-0.042589299380779266,0.3134080469608307,0.354998916387558,0.19452713429927826,-0.6118217706680298,0.8437922596931458,0.136996790766716,-0.42514804005622864,0.45009395480155945,-0.21934650838375092,0.30371081829071045,-0.04304640367627144,-0.22688913345336914,0.07385510206222534,-0.43974411487579346,0.08648283034563065,0.08647958189249039,-0.2248339205980301,0.17876288294792175,0.01872164197266102,0.055852632969617844,-0.29105493426322937,-0.4887404143810272,-0.11654383689165115,-0.4175875782966614,-0.1782689243555069,-0.2519546449184418,-0.18653610348701477,-0.150105819106102,0.09663554280996323,-2.92342209815979,0.21917638182640076,-0.27722251415252686,0.03715359419584274,0.04588858038187027,-0.07176462560892105,0.4029321074485779,-0.0397324338555336,-0.4058060646057129,0.29307466745376587,0.1803804636001587,-0.07782993465662003,-0.0574963204562664,0.3767372965812683,0.4173905849456787,0.2959991991519928,0.3768804669380188,0.010429482907056808,0.279232919216156,0.20296801626682281,-0.17206503450870514,0.23691046237945557,0.061239756643772125,-0.18568117916584015,0.12158947438001633,0.4750092625617981,-0.2707771360874176,0.018406499177217484,-0.28368648886680603,0.020001070573925972,0.06263994425535202,-0.10144626349210739,0.2017965316772461,-0.20829148590564728,-0.5910357236862183,-0.049571480602025986,0.1880379170179367,0.12105130404233932,0.4923776388168335,-0.11123669147491455,-0.2518596053123474,0.5150938630104065,0.1364137977361679,0.2886433005332947,0.35378989577293396,0.05703684687614441,-0.009235326200723648,-0.32155922055244446,0.21277615427970886,0.028364278376102448,-0.20914696156978607,-0.09293902665376663,1.1701455116271973,0.08003044128417969,-0.14993904531002045,-0.07019775360822678,0.28723469376564026,0.15694747865200043,0.2415948212146759,0.45322397351264954,0.5808707475662231,-0.3600514531135559,0.19953392446041107,-0.2867526710033417,0.10513598471879959,-0.4263027310371399,0.4745584726333618,-0.4005373418331146,0.15460972487926483,0.12081069499254227,0.3350101113319397,0.19106128811836243,-0.20238250494003296,-1.130832552909851,-0.14063848555088043,0.04963861033320427,-0.370236873626709,0.10177049785852432,0.2485460638999939,0.09459812194108963,-0.1741763949394226,-0.009227593429386616,-0.248909130692482,0.35519325733184814,-0.1372021585702896,-0.2083149254322052,0.07567049562931061,0.31308165192604065,-0.10954782366752625,-0.17579634487628937,-0.2868784964084625,0.2769846022129059,0.1145399808883667,-0.1442621499300003,-0.23449721932411194,0.0017545114969834685,0.4295244514942169,-0.9238272309303284,0.3318202793598175,-0.35402464866638184,-0.2853367328643799,-0.38989612460136414,-0.24649757146835327,0.1304006278514862,-0.09665742516517639,-0.015299828723073006,-0.25638657808303833,0.2713342010974884,-0.019852928817272186,0.4348500669002533,-0.07034722715616226,-0.21516470611095428,0.1035711020231247,0.2719975709915161,0.8460290431976318,-0.012396376579999924,-0.2726733684539795,0.2704120874404907,0.39829063415527344,0.07077965885400772,0.18103890120983124,-0.0639411136507988,-0.07781418412923813,0.010485510341823101,-0.2741599977016449,-0.06894519180059433,0.21891620755195618,-0.030977049842476845,-0.24249885976314545,-0.19899961352348328,0.1386999785900116,0.05704725161194801,-0.5700863599777222,-0.44877585768699646,0.1293371468782425,-0.321339875459671,-0.17733024060726166,0.41469335556030273,0.14530465006828308,-0.005827075336128473,0.5264756679534912,-0.14750751852989197,-0.34278038144111633,0.3775775730609894,-0.17129692435264587,0.3678739070892334,0.05970029532909393,-0.1086149737238884,-0.2936311662197113,0.3791952431201935,0.2986803948879242,-0.32161426544189453,-0.26476752758026123,-0.2352442592382431,0.14597874879837036,-0.03648101165890694,0.12330936640501022,-0.06151288375258446,-0.05225206911563873,0.11573559045791626,0.15905512869358063,0.22500012814998627,-1.5384150743484497,0.06999683380126953,0.5284489989280701,0.07390688359737396,-0.17442584037780762,-0.1290021389722824,-0.43735578656196594,0.7379924058914185,-0.2953348457813263,-0.2743741273880005,-0.2702881097793579,-0.12169115990400314,0.2569498121738434,0.058106206357479095,0.2008761316537857,0.06107509508728981,0.016121627762913704,-0.5183561444282532,-0.13178816437721252,-0.13565091788768768,-0.3364868462085724,0.4366452395915985,0.032397009432315826,0.06015865132212639,-0.0060366676189005375,-0.16685988008975983,-0.1482187807559967,0.30621537566185,-0.061259765177965164,0.07366647571325302,0.04530538618564606,-0.2882876694202423,-0.6543371677398682,-0.25789833068847656,0.09688003361225128,0.08035960048437119,0.25444501638412476,-0.048389069736003876,0.4650467038154602,0.6910762190818787,0.014812693931162357,0.567101776599884,0.233724907040596,0.06143460422754288,0.028628503903746605,0.4010844826698303,-0.4490150809288025,0.07905352860689163,-0.17245300114154816,-0.47074201703071594,-0.07310336083173752,-0.22578731179237366,-0.36511263251304626,-0.13936090469360352,0.1470455378293991,-0.1310054361820221,0.35907885432243347,0.07968833297491074,-0.1813671588897705,-0.2806437015533447,0.15874461829662323,-0.08258367329835892,-0.1939554065465927,0.14463965594768524,-0.28262171149253845,-0.8867875933647156,0.03859703615307808,-0.19753077626228333,-0.137284055352211,0.2507597506046295,0.356904536485672,-0.14984865486621857,-0.43066784739494324,-0.09739835560321808,-0.5203882455825806,0.27423739433288574,0.23854446411132812,-0.011799744330346584,0.29137465357780457,-0.18439680337905884,0.19271284341812134,-0.13809284567832947,-0.08616361767053604,0.20080025494098663,0.3176564574241638,-0.019579829648137093,-0.11548662930727005,-0.11782077699899673,-0.16307108104228973,-0.021181253716349602,-0.428952693939209,-0.3616415858268738,0.5303570628166199,0.19658561050891876,0.2811846137046814,0.008659970946609974,0.021335896104574203,0.038443680852651596,0.20669172704219818,-0.15397429466247559,-0.04260113835334778,0.503599226474762,0.058045726269483566,0.25512418150901794,0.13058462738990784,0.5298604369163513,0.27977681159973145,0.27926719188690186,-0.3363761603832245,-0.15817570686340332,-0.01802918128669262,0.2795434892177582,0.01797371730208397,0.019261695444583893,0.3575285077095032,0.0005149476346559823,0.3050927519798279,-0.26812705397605896,2.2466881275177,0.40627986192703247,0.042358893901109695,-0.40020087361335754,0.2030230611562729,0.4066832363605499,-0.04404505714774132,0.16475683450698853,-0.28084781765937805,0.26620471477508545,-0.32981076836586,0.09178374707698822,0.16507232189178467,0.13556894659996033,0.22539293766021729,0.09439016878604889,-0.25606974959373474,-0.2215113788843155,-0.8654592037200928,-0.328975647687912,-0.4663916826248169,0.6286056637763977,0.121601402759552,-0.16708777844905853,0.27309003472328186,0.4025173783302307,0.3961367607116699,0.22089308500289917,0.06968218833208084,0.26054418087005615,-0.03383089601993561,0.3003306984901428,0.071248859167099,0.11940263211727142,-0.3311759829521179,0.13880790770053864,-0.01701970584690571,-0.257216215133667,-0.16903433203697205,0.18240822851657867,0.02404913865029812,-0.27428022027015686,0.582618236541748,-0.10819101333618164,-0.2848418056964874,0.4507683515548706,-0.08422092348337173,-0.3858765661716461,0.09116045385599136,0.08655793219804764,0.37695154547691345,0.11376849561929703,-0.338549941778183,0.10398974269628525,-0.40559136867523193,-0.1424284130334854,0.4408758580684662,-0.23484763503074646,-0.22043995559215546,0.2663392424583435,-0.08561430126428604,0.5298035144805908,0.17390276491641998,-0.15414811670780182,-0.3710375130176544,-0.1377733051776886,-0.32578080892562866,0.20807422697544098,0.24836964905261993,-0.035750437527894974,-0.11850979924201965,0.5644105076789856,0.4948148727416992,0.36036452651023865,0.33101901412010193,-0.018466293811798096,0.342043399810791,-0.021328890696167946,-0.42547866702079773,-3.0606393814086914,0.13823676109313965,0.1751476675271988,0.18482959270477295,0.2119854837656021,0.42783838510513306,0.37956342101097107,-0.050889983773231506,0.19963546097278595,-0.003729148767888546,0.301714152097702,0.3647262752056122,0.2256779968738556,0.11176111549139023,-0.2000434696674347,0.14285340905189514,0.1204938217997551,-0.31672677397727966,0.08802391588687897,-0.28841838240623474,-0.2692889869213104,0.19890694320201874,0.26972270011901855,-0.13447141647338867,-0.5827397704124451,0.043538421392440796,0.04205481335520744,-0.18629205226898193,0.12292982637882233,0.3665415942668915,-0.21660615503787994,0.2598339021205902,-0.01554532628506422,0.2936937212944031,0.16579346358776093,-0.1061495915055275,-0.008095542900264263,-0.11823703348636627,0.32105347514152527,0.4089765250682831,-0.3629070818424225,0.5846423506736755,-0.18643410503864288,0.2677313983440399,-0.2738163471221924,-0.3237670063972473,0.6496291160583496,-0.2718106210231781,0.42378830909729004,-0.3160589039325714,-0.22860629856586456,0.2556455135345459,-0.07322896271944046,-0.025386868044734,0.3011501133441925,-0.244349405169487,0.21990253031253815,-0.37764087319374084,-0.177305668592453,-0.13201722502708435,0.026469293981790543,-0.20265603065490723,-0.0890762209892273,-0.12256135791540146,0.47843945026397705,-0.5051477551460266,-0.1626398265361786,0.29097047448158264,-0.31121963262557983,-0.1076011061668396,-0.19564320147037506,0.11030508577823639,0.34548255801200867,0.23899267613887787,0.09145741164684296,0.1917039155960083,0.03166079521179199,0.20480909943580627,0.46000900864601135,0.022355463355779648,-0.07255730032920837,-0.3389264941215515,-0.29332128167152405,-0.14698821306228638,0.15866436064243317,-7.905961513519287,-0.010180124081671238,-0.39888128638267517,-0.4125021696090698,0.11469347774982452,-0.009003965184092522,-0.18822722136974335,-0.16521553695201874,0.019870661199092865,-0.026155931875109673,-0.2543494701385498,0.4092269837856293,-0.06473968923091888,-0.34239548444747925,0.24469901621341705,0.6330996155738831],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Embedding Dimension\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"T5 Embeddings for the First Token\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('84ac2090-c087-4183-bd6e-0a47728739db');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line Plot"
      ],
      "metadata": {
        "id": "1zj5RpOX5rjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [2, 4, 1, 5, 3]\n",
        "\n",
        "# Create the plot\n",
        "plt.plot(x, y)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.title('Simple Line Plot')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "97KCw5wOSLPK",
        "outputId": "423c93b2-6c1d-42a0-894d-f3678d16ca9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "bYbo_BqD25ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming you have true labels and predictions from your T5 model\n",
        "y_true = [0, 1, 1, 0, 1]  # Example true labels\n",
        "y_pred = [1, 1, 0, 0, 1]  # Example predictions from T5\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "id": "gMgBY2_3SLA2",
        "outputId": "b0784435-cddc-45a6-b4e2-5ea04e1ba03c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6666666666666666\n",
            "F1-score: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Novel Improvement from above analysis**"
      ],
      "metadata": {
        "id": "mwbgqCCZ4jei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# **Novel Improvement: Ensemble Model with Weighted Voting**\n",
        "\n",
        "# 1. **Train Multiple Models:**\n",
        "#    - Train different models (e.g., BERT, GPT-2, T5) on the same dataset.\n",
        "#    - Fine-tune each model with different hyperparameters or architectures.\n",
        "\n",
        "# 2. **Generate Predictions:**\n",
        "#    - Obtain predictions from each trained model on the validation or test set.\n",
        "\n",
        "# 3. **Weighted Voting:**\n",
        "#    - Assign weights to each model based on their individual performance (e.g., F1-score, BLEU score).\n",
        "#    - Combine predictions using weighted voting, where each model's vote is weighted by its assigned weight.\n",
        "\n",
        "bert_predictions = [0, 1, 1, 0, 1]\n",
        "gpt2_predictions = [1, 1, 0, 1, 1]\n",
        "t5_predictions = [0, 1, 1, 0, 0]\n",
        "\n",
        "# Assign weights based on model performance (adjust these based on evaluation results)\n",
        "bert_weight = 0.4\n",
        "gpt2_weight = 0.3\n",
        "t5_weight = 0.3\n",
        "\n",
        "# Combine predictions using weighted voting\n",
        "ensemble_predictions = []\n",
        "for i in range(len(bert_predictions)):\n",
        "    votes = [\n",
        "        bert_predictions[i] * bert_weight,\n",
        "        gpt2_predictions[i] * gpt2_weight,\n",
        "        t5_predictions[i] * t5_weight\n",
        "    ]\n",
        "    ensemble_predictions.append(int(round(sum(votes))))\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "y_true = [1, 1, 0, 0, 1]  # Example true labels\n",
        "accuracy = accuracy_score(y_true, ensemble_predictions)\n",
        "print(\"Ensemble Accuracy:\", accuracy)\n",
        "\n",
        "# 4. **Further Refinements:**\n",
        "#    - Experiment with different weighting schemes (e.g., based on confidence scores).\n",
        "#    - Explore stacking or blending techniques to combine model predictions.\n",
        "#    - Use cross-validation to optimize model weights and hyperparameters.\n"
      ],
      "metadata": {
        "id": "FgWWCSLpSK8C",
        "outputId": "fb6c5543-a03d-4594-9471-644ec980d360",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Accuracy: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZ1OaBxWSKuT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}